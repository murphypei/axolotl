# =====================================
# Qwen3-8B LoRA 推理配置
# 用于模型推理和测试
# =====================================

# =====================================
# 模型配置
# =====================================
# 方式 1: 使用完整模型（如果已经合并了 LoRA）
# base_model: ./outputs/qwen3-8b-lora-dpo/saya_recom_reply_data-2025-12-17

# 方式 2: 使用基础模型 + LoRA 适配器（推荐，节省空间）
base_model: Qwen/Qwen3-8B
lora_model_dir: ./outputs/qwen3-8b-lora-dpo/saya_recom_reply_data-2025-12-17

# 信任远程代码（Qwen 模型需要）
trust_remote_code: true
strict: false

# =====================================
# Chat Template 配置
# =====================================
# 与训练时保持一致，确保 prompt 格式化正确
chat_template: qwen3

# 或者从数据集配置中读取（如果训练时使用了）
# datasets:
#   - type: chat_template
#     chat_template: qwen3

# =====================================
# 显存优化配置（可选）
# =====================================
# 如果显存不足，可以使用 8-bit 或 4-bit 量化
# load_in_8bit: true
# load_in_4bit: false

# =====================================
# 模型加载配置（V100 兼容性）
# =====================================
# 如果需要与训练时保持一致
fp16: true
bf16: false
flash_attention: false

# 模型初始化参数
model:
  attn_implementation: "eager"  # V100 兼容
  torch_dtype: float16
  use_cache: true  # 推理时启用 cache 提升速度

# =====================================
# Gradio 界面配置（可选，使用 --gradio 时生效）
# =====================================
gradio_title: "Qwen3-8B LoRA 推理界面"
gradio_share: false  # 是否创建公共链接
gradio_server_name: "127.0.0.1"  # 服务器地址
gradio_server_port: 7860  # 服务器端口
gradio_max_new_tokens: 1024  # 最大生成 token 数
gradio_temperature: 0.9  # 生成温度

# =====================================
# 生成参数（CLI 推理时的默认值）
# =====================================
# 注意：这些参数在 CLI 推理时是硬编码的，但可以通过修改代码或使用自定义脚本调整
# 默认值：
#   max_new_tokens: 1024
#   temperature: 0.9
#   top_p: 0.95
#   top_k: 40
#   repetition_penalty: 1.1
#   do_sample: true

# =====================================
# Tokenizer 配置（可选）
# =====================================
# tokenizer_config: path/to/tokenizer_config.json
# tokenizer_use_fast: true
# tokenizer_legacy: false

# =====================================
# 特殊 Tokens（可选）
# =====================================
# special_tokens:
#   eos_token: "<|im_end|>"
#   bos_token: "<|im_start|>"

