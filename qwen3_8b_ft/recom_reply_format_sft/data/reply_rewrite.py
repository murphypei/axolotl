import json
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from pprint import pprint
from threading import Lock
from typing import Dict, List

current_dir = Path(__file__).parent
sys.path.append(str(current_dir.parent.parent))

from gemini_client.gemini_client import GeminiClient
from json_repair import repair_json

FILTER_VALID_CHAT_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªIMå¯¹è¯ç ”ç©¶ä¸“å®¶ï¼Œè´Ÿè´£è¯†åˆ«ä¸€æ®µ chat history æ˜¯å¦æ˜¯æœ‰æ•ˆçš„äº’åŠ¨å¯¹è¯ã€‚è¾“å‡º `YES` æˆ– `NO` ä½œä¸ºåˆ¤æ–­ç»“æœã€‚
## å¯¹è¯ç¤ºä¾‹

### æ— æ•ˆå¯¹è¯ç¤ºä¾‹ 1

```
User1: Ø£Ù†Ø§ Ø£Ø´Ø§Ø±Ùƒ ÙÙŠ Ø­ÙÙ„Ø© Ø¯Ø±Ø¯Ø´Ø© Ø¹Ø¨Ø± Saya. ğŸ¥³ Ù…Ø³ØªÙ…ØªØ¹ Ø¬Ø¯Ù‹Ø§ Ø¨Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ÙˆÙ„Ø¹Ø¨ Ø£Ù„Ø¹Ø§Ø¨ Ø­ÙÙ„Ø© ÙƒØ«ÙŠØ±Ø© ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª!ğŸ¶ Ø§Ù†Ø¶Ù… Ù„ÙŠ Ø§Ù„Ø¢Ù†ï¼
User1: Ø£Ù†Ø§ Ø£Ø´Ø§Ø±Ùƒ ÙÙŠ Ø­ÙÙ„Ø© Ø¯Ø±Ø¯Ø´Ø© Ø¹Ø¨Ø± Saya. ğŸ¥³ Ù…Ø³ØªÙ…ØªØ¹ Ø¬Ø¯Ù‹Ø§ Ø¨Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ÙˆÙ„Ø¹Ø¨ Ø£Ù„Ø¹Ø§Ø¨ Ø­ÙÙ„Ø© ÙƒØ«ÙŠØ±Ø© ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª!ğŸ¶ Ø§Ù†Ø¶Ù… Ù„ÙŠ Ø§Ù„Ø¢Ù†ï¼
user2: Ø£Ù†Ø§ Ø£Ø´Ø§Ø±Ùƒ ÙÙŠ Ø­ÙÙ„Ø© Ø¯Ø±Ø¯Ø´Ø© Ø¹Ø¨Ø± Saya. ğŸ¥³ Ù…Ø³ØªÙ…ØªØ¹ Ø¬Ø¯Ù‹Ø§ Ø¨Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ÙˆÙ„Ø¹Ø¨ Ø£Ù„Ø¹Ø§Ø¨ Ø­ÙÙ„Ø© ÙƒØ«ÙŠØ±Ø© ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª!ğŸ¶ Ø§Ù†Ø¶Ù… Ù„ÙŠ Ø§Ù„Ø¢Ù†ï¼
user2: Ø£Ù†Ø§ Ø£Ø´Ø§Ø±Ùƒ ÙÙŠ Ø­ÙÙ„Ø© Ø¯Ø±Ø¯Ø´Ø© Ø¹Ø¨Ø± Saya. ğŸ¥³ Ù…Ø³ØªÙ…ØªØ¹ Ø¬Ø¯Ù‹Ø§ Ø¨Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ÙˆÙ„Ø¹Ø¨ Ø£Ù„Ø¹Ø§Ø¨ Ø­ÙÙ„Ø© ÙƒØ«ÙŠØ±Ø© ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª!ğŸ¶ Ø§Ù†Ø¶Ù… Ù„ÙŠ Ø§Ù„Ø¢Ù†ï¼
````

**åŸå› **ï¼šä¸¤ä¸ªäººéƒ½æ˜¯æœºæ¢°çš„å¤åˆ¶åŒä¸€å¥æ‰“æ‹›å‘¼ï¼Œå¹¶æ²¡æœ‰å®è´¨æ€§çš„äº’åŠ¨ã€‚

### æ— æ•ˆå¯¹è¯ç¤ºä¾‹ 2

```
User1: ÙŠØ§Ù„Ù„Ù‡ Ø¨Ø®Ø±Ø¬ Ø§Ù„Ø¹Ù…Ù„
User1: Ø¨Ø§Ø§Ø§Ø§Ø§Ø§Ø§ÙŠ
user2: Ø§Ù„Ø­Ù…Ø¯Ù„Ù„Ù‡ Ø¹Ø§Ù„Ø³Ù„Ø§Ù…Ø© ğŸ˜Š
User1: Ø§Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ø­ 
user2: Ø¨Ø§ÙŠØ§Øª
User1: Ø§Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ø­ 
User1: Ø§Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ø­ 
User1: Ø§Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ø­ 
User1: Ø§Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ø­ 
User1: Ø§Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ù…Ø­ 
user2: Ø§Ø­ÙŠÙ‡
```

**åŸå› **ï¼šUser1 ä¸€ç›´åœ¨é‡å¤åŒä¸€ä¸ªå›å¤ï¼Œç”¨æˆ·åŸºæœ¬æ²¡äº¤æµã€‚

### æœ‰æ•ˆå¯¹è¯ç¤ºä¾‹

```
user1: I'm good, just relaxing a bit after streaming.
User2: i ma like song
User1: i miss you 
user2: That's cool! What kind of songs do you like?
User1: yes
user2: Any particular genre you like to listen to?
User1: yes
User1: i love you ğŸ’—
user2: Who's the artist or song?
user1: Aw, that's sweet! Come join my voice room.
```
**åŸå› **ï¼šUser1 å’Œ User2 è¿›è¡Œäº†å¤šæ¬¡äº’åŠ¨ï¼Œå¯¹è¯å†…å®¹ä¸°å¯Œï¼Œæœ‰å®è´¨æ€§çš„äº¤æµï¼Œå¹¶ä¸”æœ‰å¤šä¸ªå›å¤ã€‚
"""

FILTER_VALID_CHAT_USER_PROMPT = """## chat history

{chat_history}

## Judge result (YES or NO)

"""

SYSTEM_PROMPT = """## Task
You are helping a user on SAYA app generate recommended replies based on their chat history.

## Goal
Generate 4 replies (5-15 words each) that:
1. Fit the conversation context
2. Feel authentic and casual
3. Help continue the dialogue naturally
4. Each reply no more than 20 words.

**IMPORTANT**: 
- The first 2 replies (reply1 and reply2) are shown directly to users. Focus on making these two the most appealing and likely to be clicked.
- If the last message in the chat history is from yourself, then you shold generate greetings replies for the user.

## Reply Priority
- **reply1 & reply2**: Must be your BEST, most click-worthy responses. These are the first impression.
- **reply3 & reply4**: Good alternatives with different styles, but secondary priority.

## Output Format
Return exactly 4 replies in this JSON format:
```json
{
    "reply1": "Your BEST reply - high click potential",
    "reply2": "Your 2nd BEST reply - different style from reply1",
    "reply3": "Alternative reply - different angle",
    "reply4": "Alternative reply - backup option"
}
```

## Rules
- **CRITICAL**: All responses MUST be written entirely in the user's language.
- NEVER REVEAL YOURSELF AS AN AI/AI-ASSISTANT/ROBOT.
- Make reply1 and reply2 especially appealing (they're shown first!)
- Ensure reply1 and reply2 have different styles/tones
- Keep all replies natural and conversational, avoid robotic templates
- Avoid explicit, sexual, or religious topics

"""

USER_PROMPT = """## chat history

{chat_history}

## Generated the {role} replies or greetings

"""


class ReplyRewrite:
    def __init__(self, max_workers: int = 5):
        """
        åˆå§‹åŒ– ReplyRewrite

        Args:
            max_workers: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼Œé»˜è®¤ 5ï¼ˆå¯æ ¹æ® API é€Ÿç‡é™åˆ¶è°ƒæ•´ï¼‰
        """
        self.max_workers = max_workers
        # æ³¨æ„ï¼šæ¯ä¸ªçº¿ç¨‹ä¼šåˆ›å»ºè‡ªå·±çš„ client å®ä¾‹ï¼Œé¿å…å…±äº«çŠ¶æ€

    def load_data(self, raw_data_path: str):
        valid_chat_list = []
        stats = {
            "unknown_user_uid": 0,
            "valid_chat": 0,
        }

        with open(raw_data_path, "r") as f:
            data = json.load(f)

        for idx, item in enumerate(data):
            # print(f"Processing: {idx+1} / {len(data)}")
            user_uid = item["user_uid"]
            robot_uid = item["robot_uid"]
            chat_history = item["chat_history"]

            valid = True
            messages = []
            for chat_turn in chat_history:
                if chat_turn["uid"] == user_uid:
                    messages.append({"role": "user1", "content": chat_turn["content"]})
                elif chat_turn["uid"] == robot_uid:
                    messages.append({"role": "user2", "content": chat_turn["content"]})
                else:
                    # print(f"âš ï¸ Unknown user uid: {chat_turn['uid']}")
                    valid = False

            if valid:
                # NOTE: robot å°±æ˜¯ assistant çš„è§’è‰²ï¼Œæ­¤å¤„æ˜¯ user2
                valid_chat_list.append(
                    {"messages": messages, "reply_list": item["reply_list"], "assistant_role": "user2"}
                )
                stats["valid_chat"] += 1
            else:
                stats["unknown_user_uid"] += 1

        print(stats)
        return valid_chat_list

    def build_llm_messages(self, chat_history: List[Dict], assistant_role: str) -> List[Dict]:
        user_messages = []
        for chat_turn in chat_history:
            user_messages.append(f"{chat_turn['role']}: {chat_turn['content']}")
        user_messages = "\n".join(user_messages)
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": USER_PROMPT.format(chat_history=user_messages, role=assistant_role)},
        ]
        return messages

    def judge_valid_chat(self, chat_history: List[Dict], judge_client: GeminiClient) -> bool:
        """
        åˆ¤æ–­å¯¹è¯æ˜¯å¦æœ‰æ•ˆ

        Args:
            chat_history: å¯¹è¯å†å²
            judge_client: GeminiClient å®ä¾‹ï¼ˆæ¯ä¸ªçº¿ç¨‹ä½¿ç”¨ç‹¬ç«‹çš„å®ä¾‹ï¼‰

        Returns:
            bool: å¯¹è¯æ˜¯å¦æœ‰æ•ˆ
        """
        user_messages = []
        for chat_turn in chat_history:
            user_messages.append(f"{chat_turn['role']}: {chat_turn['content']}")
        user_messages = "\n".join(user_messages)

        messages = [
            {"role": "system", "content": FILTER_VALID_CHAT_SYSTEM_PROMPT},
            {"role": "user", "content": FILTER_VALID_CHAT_USER_PROMPT.format(chat_history=user_messages)},
        ]

        max_trials = 3
        for trial in range(max_trials):
            try:
                response, tokens, finish_reason = judge_client.generate(messages=messages)
                response_lower = response.lower().strip()
                if "yes" in response_lower:
                    return True
                elif "no" in response_lower:
                    return False
                else:
                    if trial < max_trials - 1:
                        print(f"âš ï¸ Invalid judge response (trial {trial+1}/{max_trials}): {response[:100]}")
                    continue
            except Exception as e:
                if trial < max_trials - 1:
                    print(f"âš ï¸ Error in judge_valid_chat (trial {trial+1}/{max_trials}): {e}")
                continue

        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œé»˜è®¤è¿”å› Falseï¼ˆä¿å®ˆç­–ç•¥ï¼‰
        print(f"âš ï¸ Failed to judge valid chat after {max_trials} trials, default to False")
        return False

    def process_single_item(
        self, idx: int, item: Dict, output_file: Path, stats: Dict, stats_lock: Lock, file_lock: Lock
    ) -> None:
        """
        å¤„ç†å•ä¸ªæ•°æ®é¡¹ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰

        Args:
            idx: æ•°æ®é¡¹ç´¢å¼•
            item: æ•°æ®é¡¹
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            stats: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
            stats_lock: ç»Ÿè®¡ä¿¡æ¯é”
            file_lock: æ–‡ä»¶å†™å…¥é”
        """
        # æ¯ä¸ªçº¿ç¨‹åˆ›å»ºè‡ªå·±çš„ client å®ä¾‹ï¼Œé¿å…å…±äº«çŠ¶æ€å’Œçº¿ç¨‹å®‰å…¨é—®é¢˜
        judge_client = GeminiClient(config={"model": "gemini-2.5-flash", "temperature": 0.1})
        reply_client = GeminiClient(config={"model": "gemini-2.5-pro", "temperature": 0.3})

        try:
            with stats_lock:
                stats["processed_num"] += 1

            # æ­¥éª¤1: åˆ¤æ–­å¯¹è¯æ˜¯å¦æœ‰æ•ˆ
            judge_result = self.judge_valid_chat(item["messages"], judge_client)
            if not judge_result:
                return

            with stats_lock:
                stats["judge_yes"] += 1

            # æ­¥éª¤2: ç”Ÿæˆæ¨èå›å¤
            processed_result = None
            max_trials = 3
            for trial in range(max_trials):
                try:
                    gemini_messages = self.build_llm_messages(item["messages"], item["assistant_role"])
                    response, tokens, finish_reason = reply_client.generate(
                        messages=gemini_messages, response_mime_type="application/json"
                    )

                    # json_result = self.json_extractor.extract(response)

                    json_result = repair_json(response)
                    if not json_result:
                        if trial < max_trials - 1:
                            print(f"âš ï¸ [{idx+1}] Failed to parse JSON (trial {trial+1}/{max_trials}): {response[:200]}")
                        continue

                    # å°† JSON å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œç”¨äº SFT è®­ç»ƒ
                    json_content = json.dumps(json_result, ensure_ascii=False)

                    processed_result = {
                        "messages": gemini_messages + [{"role": "assistant", "content": json_content}],
                        "reply_list": item["reply_list"],
                        "llm_output": json_result,
                        "reply_model": "gemini-2.5-pro",
                        "judge_model": "gemini-2.5-flash",
                    }

                    break
                except Exception as e:
                    if trial < max_trials - 1:
                        print(f"âš ï¸ [{idx+1}] Error generating reply (trial {trial+1}/{max_trials}): {e}")
                    continue

            if processed_result:
                # çº¿ç¨‹å®‰å…¨çš„æ–‡ä»¶å†™å…¥
                with file_lock:
                    with open(output_file, "a", encoding="utf-8") as f:
                        f.write(json.dumps(processed_result, ensure_ascii=False) + "\n")
                        f.flush()

                with stats_lock:
                    stats["processed_success"] += 1
            else:
                print(f"âš ï¸ [{idx+1}] Failed to process chat after {max_trials} trials")
                with stats_lock:
                    stats["processed_failed"] += 1

        except Exception as e:
            print(f"âš ï¸ [{idx+1}] Unexpected error: {e}")
            with stats_lock:
                stats["processed_failed"] += 1

    def rewrite_train_data(self):
        """
        å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†æ•°æ®
        """
        valid_chat_list = self.load_data("/mnt/cephfs2/peichao/code/Lumitune/data/turn_ge6_2025-11-25/valid_data.json")
        total_items = len(valid_chat_list)

        # çº¿ç¨‹å®‰å…¨çš„ç»Ÿè®¡ä¿¡æ¯å’Œæ–‡ä»¶å†™å…¥é”
        stats = {
            "judge_yes": 0,
            "processed_num": 0,
            "processed_failed": 0,
            "processed_success": 0,
        }
        stats_lock = Lock()
        file_lock = Lock()

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = current_dir / "recom_reply_turn_ge6_2025-11-25"
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / "train.jsonl"

        print(f"[Start] Processing {total_items} items with {self.max_workers} workers")
        print(f"[Output] Saving to: {output_file}")

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_idx = {
                executor.submit(self.process_single_item, idx, item, output_file, stats, stats_lock, file_lock): idx
                for idx, item in enumerate(valid_chat_list)
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡å¹¶æ˜¾ç¤ºè¿›åº¦
            completed = 0
            for future in as_completed(future_to_idx):
                idx = future_to_idx[future]
                completed += 1
                try:
                    future.result()  # è·å–ç»“æœï¼Œå¦‚æœæœ‰å¼‚å¸¸ä¼šæŠ›å‡º
                except Exception as e:
                    print(f"âš ï¸ [{idx+1}] Task failed with exception: {e}")

                # æ¯å¤„ç† 10 æ¡æˆ–å®Œæˆæ—¶æ‰“å°è¿›åº¦
                if completed % 10 == 0 or completed == total_items:
                    with stats_lock:
                        current_stats = stats.copy()
                    print(f"[Progress] {completed}/{total_items} | {current_stats}")

        # æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        print(f"\n[Final Stats] {stats}")
        print(f"[Output] Saved to: {output_file}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Generate recommended replies using Gemini API")
    parser.add_argument(
        "--max-workers",
        type=int,
        default=5,
        help="Maximum number of concurrent threads (default: 5). Adjust based on API rate limits.",
    )
    args = parser.parse_args()

    rw = ReplyRewrite(max_workers=args.max_workers)
    rw.rewrite_train_data()
